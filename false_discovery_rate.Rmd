---
title: "Assignment 1"
author: "Meng-Wei Wu"
date: '2023-01-09'
output: html_document
---

```{r}
library(readr)
library(dplyr)
library(ggplot2)
library(leaps)
library(fastDummies)
library(car)
```

## Question 1
[5 pts] Write code that produces a 10,000 x 1001 matrix (rows x cols) of random numbers drawn from N(0,1). Seed your code using the last 4 digits of your phone number (this number will be different for everyone).  Every time you run the code, it should now yield the exact same (“random”) dataset.
```{r}
set.seed(3531)
data <- rnorm(10010000)
matrix <- matrix(data, nrow = 10000, ncol = 1001)
colnames(matrix) <- 1:1001
hist(matrix)
```

## Question 2
[5 pts] Treat the first column as “y” and the remaining 1000 columns as x’s.
```{r}
y <- matrix[, 1]
num <- 1
for (num in 1:1000){
  assign(paste("x", num, sep = ""), matrix[, num + 1])
  num = num + 1
}
```

## Question 3
[15 pts] Regress y on x’s. Is an intercept needed?  Why?  Why not?
```{r}
variables <- paste("x", 1:1000, sep = "")
formula1 <- paste("y", paste(variables, collapse = "+"), sep = "~")
formula2 <- paste(formula1, "-1", sep = "")

# with intercept
model1 <- lm(formula1)
summary(model1)

# without intercept
model2 <- lm(formula2)
summary(model2)
```

### Comment
In my opinion, I think an intercept is not needed. We can remove the intercept because the data is generated based on the normal distribution. The mean of the y and x should be zero, so the regression line must pass through the origin. Therefore, we should remove the intercept in this case.

## Question 4
[5 pts] Create a histogram of the p-values from the regression in Q3. What distribution does this histogram look like?
```{r}
pvalues <- summary(model2)$coefficients[, 4]
hist(pvalues, main = "Histogram of the p-values" , xlab = "p-value")
```

### Comment
Based on the graph, the histogram looks like the uniform distribution.

## Question 5
[15 pts] How many “significant” variables do you expect to find knowing how the data was generated? How many “significant” variables does the regression yield if alpha = 0.01?  What does this tell us?
```{r}
# alpha = 0.01
i <- 1
alpha1 <- 0.01
count1 <- 0
for (i in 1:1000) {
  if (pvalues[i] < alpha1){
    count1 = count1 + 1
    i = i + 1
  }
  i = i + 1  
}
count1
```

### Comment
In my opinion, there should be no significant variables in the regression because the data in this case is generated randomly.
If alpha = 0.01, the regression model yields approximately 8 significant variables. 

From this, we can find that even though we know the model should not contain any significant variables when we know how the data is generated, there are still several significant variables, which is the problem of false discovery. Therefore, we should use the Benjamini-Hochberg procedure to control and verify it.

## Question 6
[10 pts] Given the p values you find, use the BH procedure to control the FDR with a q of 0.1. How many “true” discoveries do you estimate?
```{r}
pvalues_df <- data.frame(pvalues)
q <- 0.1
n <- 1001
pvalues_df <- pvalues_df %>%
  arrange(pvalues) %>%
  mutate(rank = 1:1000) %>%
  mutate(BHvalues = q * rank / n) %>%
  mutate(test = ifelse(pvalues <= BHvalues, "TRUE", "FALSE"))
head(pvalues_df, 15)
```

### Comment
Based on the result of the BH procedure, we can know that there is no 'true' discovery in this case as every p-value is not smaller than their respective BH values when the q is 0.1. This result is consistent with my initial opinion.

## Question 7
[5 pts] Explore the “autos.csv” data. Include any metrics and / or plots you find interesting.
```{r}
autos <- read_csv("autos.csv")
# summary for all variables
## numeric data
summary(autos)

## categorical data
# make
ggplot(aes(x = make), data = autos) + geom_bar() + coord_flip()

# fuel_type
ggplot(aes(x = fuel_type), data = autos) + geom_bar()

# aspiration
ggplot(aes(x = aspiration), data = autos) + geom_bar()

# num_of_doors
ggplot(aes(x = num_of_doors), data = autos) + geom_bar()

# body_style
ggplot(aes(x = body_style), data = autos) + geom_bar()

# drive_wheels
ggplot(aes(x = drive_wheels), data = autos) + geom_bar()

# engine_location
ggplot(aes(x = engine_location), data = autos) + geom_bar()

# engine_type
ggplot(aes(x = engine_type), data = autos) + geom_bar()

# fuel_system
ggplot(aes(x = fuel_system), data = autos) + geom_bar()
```

## Question 8
[15 pts] Create a linear regression model to predict price. Explain your model.
```{r}
# create dummy variables
autos_df <- dummy_cols(autos, select_columns = c("make", "fuel_type", "aspiration", "num_of_doors", "body_style", "drive_wheels", "engine_location", "engine_type", "fuel_system"), remove_selected_columns = TRUE, remove_first_dummy = TRUE)

# create the model with all variables
model3 <- lm(price ~ ., data = autos_df)
summary(model3)

# remove the variables which have singularity problem
autos_modified1 <- subset(autos, select = -c(fuel_system, engine_type))
autos_modified1_df <- dummy_cols(autos_modified1, select_columns = c("make", "fuel_type", "aspiration", "num_of_doors", "body_style", "drive_wheels", "engine_location"), remove_selected_columns = TRUE, remove_first_dummy = TRUE)

model3_modified1 <- lm(price ~ ., data = autos_modified1_df)
summary(model3_modified1) 

# check the multicolinearity problem by using VIF function and remove the variables with extremely high VIF
vif(model3_modified1)

autos_modified2 <- subset(autos_modified1, select = -c(compression_ratio, fuel_type))
autos_modified2_df <- dummy_cols(autos_modified2, select_columns = c("make", "aspiration", "num_of_doors", "body_style", "drive_wheels", "engine_location"), remove_selected_columns = TRUE, remove_first_dummy = TRUE)

model3_modified2 <- lm(price ~ ., data = autos_modified2_df)
summary(model3_modified2) 

# alpha = 0.01
pvalues_autos <- summary(model3_modified2)$coefficient[, 4]
j <- 1
alpha2 <- 0.1
count2 <- 0
for (j in 1:length(pvalues_autos)) {
  if (pvalues_autos[j] < alpha2){
    count2 = count2 + 1
    j = j + 1
  }
  j = j + 1  
}
count2
```

### Comment
First of all, I ran the model containing all the numeric and categorical(dummy) variables to test whether there is a singularity problem and removed the variables('fuel_system' and 'engine_type') to solve this issue. Then, I calculated the VIF for each variables in the second model, removed the variables('compression_ratio' and 'fuel_type') with extremely high VIF and created the third model again. From the final model(the third model), we have 20 significant variables at 10% significance level and the overall validity of the model is statistically significant.

## Question 9
[10 pts] Why might false discoveries be an issue?

### Comment
False discovery is an issue because this may mislead the interpretation of a model we created. If we create a model which contains over 10% false significant variables but we do not recognize it, it will lead to a different or even an opposite conclusion or forecast based on the model. Therefore, we should set an acceptable false discovery rate and run the BH procedure to mitigate this issue.

## Question 10
[15 pts] Use the BH procedure to control the FDR with a q of 0.1. How many true discoveries do you estimate? Plot the cutoff line together with the significant and insignificant p-values.
```{r}
pvalues_autos_df <- data.frame(pvalues_autos)
q <- 0.1
n <- nrow(pvalues_autos_df)
pvalues_autos_df <- pvalues_autos_df %>%
  arrange(pvalues_autos) %>%
  mutate(rank = 1:n) %>%
  mutate(BHvalues = q * rank / n) %>%
  mutate(test = ifelse(pvalues_autos <= BHvalues, "TRUE", "FALSE"))
head(pvalues_autos_df, 15)

# count significant values
k <- 1
count3 <- 0
for (k in 1:nrow(pvalues_autos_df)) {
  if (pvalues_autos_df$test[k] == "TRUE") {
    count3 = count3 + 1
    k = k + 1
  }
  k = k + 1  
}
count3

plot(pvalues_autos_df$pvalues_autos, log = "xy", col = ifelse(pvalues_autos_df$test == "TRUE", "red", "grey"), pch = 19, xlab = "rank ordered by p-values", ylab = "p-values", main = paste("FDR =", q))
lines(1:n, q*(1:n) / n)

# fdr (codes from Professor)
fdr <- function(pvals, q, plotit=FALSE){
  pvals <- pvals[!is.na(pvals)]
  N <- length(pvals)
  
  k <- rank(pvals, ties.method="min")
  alpha <- max(pvals[ pvals <= (q*k/N) ])
  
  if(plotit){
    sig <- factor(pvals <= alpha)
    o <- order(pvals)
    plot(pvals[o], log="xy", col=c("grey60","red")[sig[o]], pch=20, 
      ylab="p-values", xlab="tests ordered by p-value", main = paste('FDR =',q))
    lines(1:N, q*(1:N) / N)
  }
  
  return(alpha)
}

# plot the variables
fdr(pvalues_autos, 0.1, plotit = TRUE)
```

### Comment
After using BH procedure with q = 0.1, which means the false discovery rate is 0.1, I estimate that we should have at least 16 * (1 - 0.1) = 14.4 true significant variables from the regression model.
